{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"churndata/test.csv\")\n",
    "train_data = pd.read_csv(\"churndata/training.csv\")\n",
    "unseen_data = pd.read_csv(\"churndata/unseen.csv\")\n",
    "\n",
    "X_train = train_data.drop([\"Churn?\"], axis=1)\n",
    "X_test = test_data.drop([\"Churn?\"], axis=1)\n",
    "X_unseen = unseen_data.drop([\"Churn?\"], axis=1)\n",
    "\n",
    "y_train = train_data[\"Churn?\"]\n",
    "y_test = test_data[\"Churn?\"]\n",
    "y_unseen = unseen_data[\"Churn?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n_estimators is the number of bootstrapped decision trees\n",
    "# that will be produced\n",
    "clf = RandomForestClassifier(n_estimators=40)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(f'Train Accuracy: {clf.score(X_train, y_train)}')\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test)}')\n",
    "print(f'Unseen Accuracy: {clf.score(X_unseen, y_unseen)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "df_importances = forest_importances.to_frame()\n",
    "\n",
    "df_importances = df_importances.rename(columns={0: \"importance\"})\n",
    "df_importances = df_importances.sort_values(by='importance')\n",
    "df_importances.plot.bar(legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_predict\n",
    "y_predicted = clf.predict(X_test)\n",
    "# y_predicted = cross_val_predict(model, X_test, y_test, cv=3) #cv is ilang k-fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predicted, labels=[0, 1])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_predicted)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
